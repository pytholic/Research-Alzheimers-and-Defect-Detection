{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from blocks import residual_block\n",
    "from blocks import attention_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AttentionResNet92(shape=(224, 224, 3), n_channels=64, n_classes=100,\n",
    "                      dropout=0, regularization=0.01):\n",
    "    \"\"\"\n",
    "    Attention-92 ResNet\n",
    "    https://arxiv.org/abs/1704.06904\n",
    "    \"\"\"\n",
    "    regularizer = l2(regularization)\n",
    "\n",
    "    input_ = Input(shape=shape)\n",
    "    x = Conv2D(n_channels, (7, 7), strides=(2, 2), padding='same')(input_) # 112x112\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)  # 56x56\n",
    "\n",
    "    x = residual_block(x, output_channels=n_channels * 4)  # 56x56\n",
    "    x = attention_block(x, encoder_depth=3)  # bottleneck 7x7\n",
    "\n",
    "    x = residual_block(x, output_channels=n_channels * 8, stride=2)  # 28x28\n",
    "    x = attention_block(x, encoder_depth=2)  # bottleneck 7x7\n",
    "    x = attention_block(x, encoder_depth=2)  # bottleneck 7x7\n",
    "\n",
    "    x = residual_block(x, output_channels=n_channels * 16, stride=2)  # 14x14\n",
    "    x = attention_block(x, encoder_depth=1)  # bottleneck 7x7\n",
    "    x = attention_block(x, encoder_depth=1)  # bottleneck 7x7\n",
    "    x = attention_block(x, encoder_depth=1)  # bottleneck 7x7\n",
    "\n",
    "    x = residual_block(x, output_channels=n_channels * 32, stride=2)  # 7x7\n",
    "    x = residual_block(x, output_channels=n_channels * 32)\n",
    "    x = residual_block(x, output_channels=n_channels * 32)\n",
    "\n",
    "    pool_size = (x.get_shape()[1].value, x.get_shape()[2].value)\n",
    "    x = AveragePooling2D(pool_size=pool_size, strides=(1, 1))(x)\n",
    "    x = Flatten()(x)\n",
    "    if dropout:\n",
    "        x = Dropout(dropout)(x)\n",
    "    output = Dense(n_classes, kernel_regularizer=regularizer, activation='sigmoid')(x) # softmax\n",
    "\n",
    "    model = Model(input_, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AttentionResNet56(shape=(224, 224, 3), n_channels=64, n_classes=100,\n",
    "                      dropout=0, regularization=0.01):\n",
    "    \"\"\"\n",
    "    Attention-56 ResNet\n",
    "    https://arxiv.org/abs/1704.06904\n",
    "    \"\"\"\n",
    "\n",
    "    regularizer = l2(regularization)\n",
    "\n",
    "    input_ = Input(shape=shape)\n",
    "    x = Conv2D(n_channels, (7, 7), strides=(2, 2), padding='same')(input_) # 112x112\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)  # 56x56\n",
    "\n",
    "    x = residual_block(x, output_channels=n_channels * 4)  # 56x56\n",
    "    x = attention_block(x, encoder_depth=3)  # bottleneck 7x7\n",
    "\n",
    "    x = residual_block(x, output_channels=n_channels * 8, stride=2)  # 28x28\n",
    "    x = attention_block(x, encoder_depth=2)  # bottleneck 7x7\n",
    "\n",
    "    x = residual_block(x, output_channels=n_channels * 16, stride=2)  # 14x14\n",
    "    x = attention_block(x, encoder_depth=1)  # bottleneck 7x7\n",
    "\n",
    "    x = residual_block(x, output_channels=n_channels * 32, stride=2)  # 7x7\n",
    "    x = residual_block(x, output_channels=n_channels * 32)\n",
    "    x = residual_block(x, output_channels=n_channels * 32)\n",
    "\n",
    "    pool_size = (x.get_shape()[1].value, x.get_shape()[2].value)\n",
    "    x = AveragePooling2D(pool_size=pool_size, strides=(1, 1))(x)\n",
    "    x = Flatten()(x)\n",
    "    if dropout:\n",
    "        x = Dropout(dropout)(x)\n",
    "    output = Dense(n_classes, kernel_regularizer=regularizer, activation='sigmoid')(x) # softmax\n",
    "\n",
    "    model = Model(input_, output)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
